# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¹ã‚¤ãƒ¼ãƒ—ã‚¬ã‚¤ãƒ‰

## æ¦‚è¦

å…¨ã¦ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®çµ„ã¿åˆã‚ã›ã§å®Ÿé¨“ã‚’è‡ªå‹•çš„ã«å®Ÿè¡Œã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã™ã€‚

### æ¢ç´¢ã™ã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

- **data.alpha**: [0.1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] (11ç¨®é¡)
- **training.lr**: [0.001, 0.00001, 0.000001] (3ç¨®é¡)
- **lora.dropout**: [0.1, 0.2, 0.3] (3ç¨®é¡)

**åˆè¨ˆ**: 11 Ã— 3 Ã— 3 = **99é€šã‚Šã®å®Ÿé¨“**

## ä½¿ã„æ–¹

### æ–¹æ³•1: é€æ¬¡å®Ÿè¡Œï¼ˆæ¨å¥¨ï¼šãƒ¡ãƒ¢ãƒªåˆ¶ç´„ãŒã‚ã‚‹å ´åˆï¼‰

```bash
python run_hyperparameter_sweep.py
```

**ç‰¹å¾´**:
- 1ã¤ãšã¤é †ç•ªã«å®Ÿé¨“ã‚’å®Ÿè¡Œ
- GPU ãƒ¡ãƒ¢ãƒªã®ä½¿ç”¨é‡ãŒå°‘ãªã„
- å®Ÿè¡Œæ™‚é–“ãŒé•·ã„ï¼ˆ99å®Ÿé¨“ Ã— å®Ÿé¨“æ™‚é–“ï¼‰
- é€”ä¸­çµŒéãŒãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§è¡¨ç¤ºã•ã‚Œã‚‹

### æ–¹æ³•2: ä¸¦åˆ—å®Ÿè¡Œï¼ˆæ¨å¥¨ï¼šè¤‡æ•°GPUåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰

```bash
python run_hyperparameter_sweep_parallel.py
```

**ç‰¹å¾´**:
- è¤‡æ•°ã®å®Ÿé¨“ã‚’ä¸¦åˆ—å®Ÿè¡Œ
- GPUè‡ªå‹•æ¤œå‡ºï¼ˆ3æšãªã‚‰3ä¸¦åˆ—ã€4æšãªã‚‰4ä¸¦åˆ—ï¼‰
- å®Ÿè¡Œæ™‚é–“ãŒå¤§å¹…ã«çŸ­ç¸®ï¼ˆæœ€å¤§ã§ä¸¦åˆ—æ•°å€é€Ÿï¼‰
- è¤‡æ•°GPUã¾ãŸã¯å¤§å®¹é‡GPUãƒ¡ãƒ¢ãƒªãŒå¿…è¦

### æ–¹æ³•3: nohup ã§ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å®Ÿè¡Œï¼ˆæ¨å¥¨ï¼šé•·æ™‚é–“å®Ÿé¨“ï¼‰

SSHåˆ‡æ–­å¾Œã‚‚å®Ÿé¨“ã‚’ç¶™ç¶šã—ãŸã„å ´åˆã«æœ€é©ã§ã™ã€‚

#### ç°¡å˜ãªå®Ÿè¡Œæ–¹æ³•ï¼ˆãƒ˜ãƒ«ãƒ‘ãƒ¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½¿ç”¨ï¼‰

```bash
# é€æ¬¡å®Ÿè¡Œã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§
./run_sweep_nohup.sh sequential

# ä¸¦åˆ—å®Ÿè¡Œã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§
./run_sweep_nohup.sh parallel
```

#### æ‰‹å‹•ã§ã® nohup å®Ÿè¡Œ

```bash
# é€æ¬¡å®Ÿè¡Œ
nohup python -u run_hyperparameter_sweep.py > logs/sweep/sweep_$(date +%Y%m%d_%H%M%S).log 2>&1 &

# ä¸¦åˆ—å®Ÿè¡Œ
nohup python -u run_hyperparameter_sweep_parallel.py > logs/sweep/sweep_parallel_$(date +%Y%m%d_%H%M%S).log 2>&1 &

# PIDã‚’ç¢ºèª
echo $!
```

**é‡è¦**: `-u` ãƒ•ãƒ©ã‚°ã¯ Python ã®å‡ºåŠ›ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ã‚’ç„¡åŠ¹åŒ–ã—ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ãƒ­ã‚°ã«è¨˜éŒ²ã—ã¾ã™ã€‚

**ä¸¦åˆ—æ•°ã®èª¿æ•´**:

ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ç·¨é›†ã—ã¦`NUM_PARALLEL_JOBS`ã‚’å¤‰æ›´ï¼š

```python
# ä¸¦åˆ—æ•°ã‚’å¤‰æ›´ï¼ˆGPUãƒ¡ãƒ¢ãƒªã«å¿œã˜ã¦èª¿æ•´ï¼‰
NUM_PARALLEL_JOBS = 2  # ä¾‹: 2ä¸¦åˆ—
NUM_PARALLEL_JOBS = 4  # ä¾‹: 4ä¸¦åˆ—ï¼ˆå¤§å®¹é‡GPUã®å ´åˆï¼‰
```

## å‡ºåŠ›çµæœ

### ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

```
experiments/hyperparameter_sweep/
â””â”€â”€ 20251001_123456/                    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    â”œâ”€â”€ sweep_summary.json              # å…¨å®Ÿé¨“ã®æ¦‚è¦
    â”œâ”€â”€ results_summary.csv             # çµæœä¸€è¦§ï¼ˆCSVå½¢å¼ï¼‰
    â”œâ”€â”€ alpha0.1_lr0.001_dropout0.1/   # å€‹åˆ¥å®Ÿé¨“1
    â”‚   â”œâ”€â”€ config.yaml
    â”‚   â”œâ”€â”€ training.log
    â”‚   â””â”€â”€ final_results_*.json
    â”œâ”€â”€ alpha0.1_lr0.001_dropout0.2/   # å€‹åˆ¥å®Ÿé¨“2
    â”‚   â””â”€â”€ ...
    â””â”€â”€ ...
```

### çµæœã®ç¢ºèª

#### 1. JSONå½¢å¼ã®æ¦‚è¦

```bash
cat experiments/hyperparameter_sweep/TIMESTAMP/sweep_summary.json
```

å†…å®¹:
- å…¨å®Ÿé¨“ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ï¼ˆæˆåŠŸ/å¤±æ•—ï¼‰
- å„å®Ÿé¨“ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- å®Ÿè¡Œæ™‚é–“
- çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

#### 2. CSVå½¢å¼ã®çµæœä¸€è¦§

```bash
cat experiments/hyperparameter_sweep/TIMESTAMP/results_summary.csv
```

åˆ—:
- `experiment_id`: å®Ÿé¨“ç•ªå·
- `alpha`: ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- `lr`: å­¦ç¿’ç‡
- `dropout`: LoRAãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡
- `status`: å®Ÿé¨“ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
- `duration_hours`: å®Ÿè¡Œæ™‚é–“ï¼ˆæ™‚é–“ï¼‰
- `best_test_accuracy`: æœ€é«˜ãƒ†ã‚¹ãƒˆç²¾åº¦
- `final_avg_accuracy`: æœ€çµ‚å¹³å‡ç²¾åº¦

#### 3. ãƒˆãƒƒãƒ—10ã®çµæœ

ã‚¹ã‚¯ãƒªãƒ—ãƒˆçµ‚äº†æ™‚ã«è‡ªå‹•çš„ã«è¡¨ç¤ºã•ã‚Œã¾ã™ï¼š

```
ğŸ† Top 10 Best Results:
  alpha    lr  dropout  best_test_accuracy
    5.0 0.001      0.2               75.32
    3.0 0.001      0.1               74.89
   ...
```

## å®Ÿè¡Œæ™‚ã®æ³¨æ„ç‚¹

### å®Ÿè¡Œæ™‚é–“ã®è¦‹ç©ã‚‚ã‚Š

- **1å®Ÿé¨“ã‚ãŸã‚Šã®æ™‚é–“**: ç´„1-2æ™‚é–“ï¼ˆ100ãƒ©ã‚¦ãƒ³ãƒ‰ã®å ´åˆï¼‰
- **é€æ¬¡å®Ÿè¡Œ**: 99å®Ÿé¨“ Ã— 1.5æ™‚é–“ = ç´„150æ™‚é–“ï¼ˆ6æ—¥ï¼‰
- **2ä¸¦åˆ—å®Ÿè¡Œ**: 99å®Ÿé¨“ Ã· 2 Ã— 1.5æ™‚é–“ = ç´„75æ™‚é–“ï¼ˆ3æ—¥ï¼‰
- **4ä¸¦åˆ—å®Ÿè¡Œ**: 99å®Ÿé¨“ Ã· 4 Ã— 1.5æ™‚é–“ = ç´„38æ™‚é–“ï¼ˆ1.5æ—¥ï¼‰

### GPU ãƒ¡ãƒ¢ãƒªè¦ä»¶

- **1å®Ÿé¨“ã‚ãŸã‚Š**: ç´„4-8GBï¼ˆãƒ¢ãƒ‡ãƒ«ã¨ãƒãƒƒãƒã‚µã‚¤ã‚ºã«ã‚ˆã‚‹ï¼‰
- **é€æ¬¡å®Ÿè¡Œ**: 1GPUï¼ˆ4-8GBï¼‰ã§å¯èƒ½
- **2ä¸¦åˆ—å®Ÿè¡Œ**: 2GPU ã¾ãŸã¯ 1GPUï¼ˆ16GBä»¥ä¸Šï¼‰
- **4ä¸¦åˆ—å®Ÿè¡Œ**: 4GPU ã¾ãŸã¯ 2GPUï¼ˆ16GBä»¥ä¸Šï¼‰

### ä¸­æ–­ã¨å†é–‹

#### ä¸­æ–­æ–¹æ³•

```bash
Ctrl+C  # ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’ä¸­æ–­
```

#### å†é–‹æ–¹æ³•

ç¾åœ¨ã€è‡ªå‹•å†é–‹æ©Ÿèƒ½ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ä»¥ä¸‹ã®æ‰‹é †ã§æ‰‹å‹•å†é–‹ï¼š

1. `sweep_summary.json`ã‹ã‚‰æˆåŠŸã—ãŸå®Ÿé¨“ã‚’ç¢ºèª
2. ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ç·¨é›†ã—ã¦ã€æœªå®Ÿè¡Œã®çµ„ã¿åˆã‚ã›ã®ã¿ã‚’è¨­å®š
3. å†å®Ÿè¡Œ

## ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ç·¨é›†ã—ã¦æ¢ç´¢ç¯„å›²ã‚’å¤‰æ›´ã§ãã¾ã™ï¼š

```python
# run_hyperparameter_sweep.py ã‚’ç·¨é›†

# æ¢ç´¢ç¯„å›²ã‚’å¤‰æ›´
ALPHA_VALUES = [0.1, 0.5, 1.0, 5.0, 10.0]  # 5ç¨®é¡ã«å‰Šæ¸›
LR_VALUES = [0.001, 0.0001]                 # 2ç¨®é¡ã«å‰Šæ¸›
DROPOUT_VALUES = [0.1, 0.2]                 # 2ç¨®é¡ã«å‰Šæ¸›

# ã“ã®å ´åˆã€åˆè¨ˆ 5 Ã— 2 Ã— 2 = 20å®Ÿé¨“
```

## ãƒ™ãƒ¼ã‚¹è¨­å®šã®å¤‰æ›´

åˆ¥ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆï¼š

```python
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ç·¨é›†
BASE_CONFIG = "configs/experiment_configs_iid/IID-FedSA-LoRA.yaml"
```

ã¾ãŸã¯ã€ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’è¿½åŠ ã™ã‚‹ï¼ˆå°†æ¥çš„ãªæ‹¡å¼µï¼‰ã€‚

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼

```
CUDA out of memory
```

**è§£æ±ºç­–**:
1. ä¸¦åˆ—æ•°ã‚’æ¸›ã‚‰ã™ï¼ˆ`NUM_PARALLEL_JOBS`ã‚’1ã«è¨­å®šï¼‰
2. ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¸›ã‚‰ã™ï¼ˆãƒ™ãƒ¼ã‚¹è¨­å®šã®`data.batch_size`ã‚’32ã«å‰Šæ¸›ï¼‰
3. ã‚ˆã‚Šå¤§ããªGPUã‚’ä½¿ç”¨

### å®Ÿé¨“ãŒå¤±æ•—ã™ã‚‹

1. ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªï¼š
   ```bash
   tail -n 50 experiments/hyperparameter_sweep/TIMESTAMP/alphaX_lrY_dropoutZ/training.log
   ```

2. å€‹åˆ¥ã«å®Ÿé¨“ã‚’å®Ÿè¡Œã—ã¦ãƒ‡ãƒãƒƒã‚°ï¼š
   ```bash
   python quickstart_resnet.py --config experiments/hyperparameter_sweep/TIMESTAMP/alphaX_lrY_dropoutZ/config.yaml
   ```

### ãƒ—ãƒ­ã‚»ã‚¹ã®ç›£è¦–

#### ãƒ˜ãƒ«ãƒ‘ãƒ¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ç¢ºèªï¼ˆæ¨å¥¨ï¼‰

```bash
# å®Ÿè¡ŒçŠ¶æ³ã‚’ç¢ºèª
./check_sweep_status.sh

# ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ãƒ­ã‚°ã‚’ç›£è¦–
tail -f logs/sweep/sweep_*.log

# å®Ÿé¨“ã‚’åœæ­¢
./stop_sweep.sh
```

#### æ‰‹å‹•ã§ã®ç›£è¦–

```bash
# GPUä½¿ç”¨ç‡ã®ç›£è¦–
watch -n 1 nvidia-smi

# ãƒ—ãƒ­ã‚»ã‚¹ã®ç¢ºèª
ps aux | grep run_hyperparameter_sweep

# ãƒ­ã‚°ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤º
tail -f logs/sweep/sweep_*.log

# å®Œäº†ã—ãŸå®Ÿé¨“æ•°ã‚’ç¢ºèª
find experiments/hyperparameter_sweep/TIMESTAMP -name "final_results_*.json" | wc -l

# ãƒ—ãƒ­ã‚»ã‚¹ã‚’åœæ­¢ï¼ˆPIDãŒ12345ã®å ´åˆï¼‰
kill 12345
```

## çµæœã®åˆ†æ

### Pythonã§ã®åˆ†æä¾‹

```python
import pandas as pd
import matplotlib.pyplot as plt

# çµæœã®èª­ã¿è¾¼ã¿
df = pd.read_csv('experiments/hyperparameter_sweep/TIMESTAMP/results_summary.csv')

# æˆåŠŸã—ãŸå®Ÿé¨“ã®ã¿
df_success = df[df['status'] == 'SUCCESS']

# æœ€è‰¯ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¦‹ã¤ã‘ã‚‹
best_row = df_success.loc[df_success['best_test_accuracy'].idxmax()]
print(f"Best config: alpha={best_row['alpha']}, lr={best_row['lr']}, dropout={best_row['dropout']}")
print(f"Best accuracy: {best_row['best_test_accuracy']:.2f}%")

# ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã®ä½œæˆï¼ˆalpha vs lrã€dropoutå›ºå®šï¼‰
for dropout in df_success['dropout'].unique():
    subset = df_success[df_success['dropout'] == dropout]
    pivot = subset.pivot_table(values='best_test_accuracy', 
                               index='alpha', 
                               columns='lr')
    
    plt.figure(figsize=(10, 6))
    plt.imshow(pivot.values, cmap='viridis', aspect='auto')
    plt.colorbar(label='Best Test Accuracy (%)')
    plt.xlabel('Learning Rate')
    plt.ylabel('Alpha')
    plt.title(f'Accuracy Heatmap (Dropout={dropout})')
    plt.xticks(range(len(pivot.columns)), pivot.columns)
    plt.yticks(range(len(pivot.index)), pivot.index)
    plt.tight_layout()
    plt.savefig(f'heatmap_dropout_{dropout}.png')
    plt.close()
```

## ãƒ˜ãƒ«ãƒ‘ãƒ¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

### `run_sweep_nohup.sh`
ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ sweep ã‚’é–‹å§‹

```bash
./run_sweep_nohup.sh sequential   # é€æ¬¡å®Ÿè¡Œ
./run_sweep_nohup.sh parallel     # ä¸¦åˆ—å®Ÿè¡Œ
```

### `check_sweep_status.sh`
å®Ÿè¡Œä¸­ã® sweep ã®çŠ¶æ³ã‚’ç¢ºèª

```bash
./check_sweep_status.sh
```

å‡ºåŠ›ä¾‹ï¼š
```
âœ… Parallel sweep is RUNNING (PID: 12345)
   Process info: 12345 01:23:45 25.3 2.1 python run_hyperparameter_sweep_parallel.py
   Latest log: logs/sweep/sweep_parallel_20251001_120000.log
   
Completed Experiments:
20251001_120000: 15/99 experiments completed

GPU Usage:
GPU 0 (NVIDIA RTX 3090): 95% util, 8192MB / 24576MB
GPU 1 (NVIDIA RTX 3090): 94% util, 8105MB / 24576MB
GPU 2 (NVIDIA RTX 3090): 96% util, 8234MB / 24576MB
```

### `stop_sweep.sh`
å®Ÿè¡Œä¸­ã® sweep ã‚’åœæ­¢

```bash
./stop_sweep.sh
```

## ã‚ˆãã‚ã‚‹è³ªå•

### Q: SSHåˆ‡æ–­å¾Œã‚‚å®Ÿé¨“ã‚’ç¶šã‘ãŸã„
A: `./run_sweep_nohup.sh` ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

### Q: å®Ÿé¨“ã®é€²æ—ã‚’ç¢ºèªã—ãŸã„
A: `./check_sweep_status.sh` ã¾ãŸã¯ `tail -f logs/sweep/sweep_*.log` ã‚’ä½¿ç”¨ã€‚

### Q: å®Ÿé¨“ã‚’é€”ä¸­ã§æ­¢ã‚ãŸã„
A: `./stop_sweep.sh` ã¾ãŸã¯æ‰‹å‹•ã§ `kill <PID>` ã‚’å®Ÿè¡Œã€‚

### Q: ä¸¦åˆ—å®Ÿè¡Œã§ GPU æ•°ã‚’åˆ¶é™ã—ãŸã„
A: `run_hyperparameter_sweep_parallel.py` ã® `NUM_PARALLEL_JOBS = 2` ã‚’ç·¨é›†ã€‚

### Q: å®Ÿé¨“ãŒå¤±æ•—ã—ãŸçµ„ã¿åˆã‚ã›ã ã‘ã‚’å†å®Ÿè¡Œã—ãŸã„
A: `sweep_summary.json` ã‚’ç¢ºèªã—ã¦ã€å¤±æ•—ã—ãŸå®Ÿé¨“ã®è¨­å®šã‚’æŠ½å‡ºã—ã€æ–°ã—ã„ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã€‚

## å‚è€ƒ

- ãƒ™ãƒ¼ã‚¹è¨­å®š: `configs/experiment_configs_non_iid/non-IID-FedSA-LoRA.yaml`
- ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ: `quickstart_resnet.py`
- çµæœãƒ—ãƒ­ãƒƒãƒˆ: `plot_results.py`ï¼ˆæ—¢å­˜ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼‰
