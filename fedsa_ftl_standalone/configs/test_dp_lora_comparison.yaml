# Test configuration for DP-LoRA comparison studies
# This config is designed for testing IID vs Non-IID with different alpha values

# Random seed for reproducibility
seed: 42

# Device configuration
use_gpu: true

# Data configuration - will be overridden by command line
data:
  dataset_name: cifar100
  data_dir: ./data
  num_clients: 10
  data_split: non_iid  # 'iid' or 'non_iid'
  alpha: 0.5  # Will be varied: 10.0 (IID), 0.5, 0.1 (more heterogeneous)
  batch_size: 64
  num_workers: 4
  model_type: resnet
  verbose: true  # Show data distribution for analysis
  
  # Minimal augmentation for fair comparison
  augmentations:
    horizontal_flip:
      enabled: true
      prob: 0.5
    
    random_rotation:
      enabled: false
    
    color_jitter:
      enabled: false
    
    random_crop:
      enabled: false
    
    random_erasing:
      enabled: false
    
    mixup:
      enabled: false
    
    cutmix:
      enabled: false

# Model configuration - lightweight for faster testing
model:
  model_name: resnet18  # Use ResNet18 for faster experiments
  num_classes: 100
  pretrained: true
  
  # LoRA configuration
  lora_r: 8  # Standard rank
  lora_alpha: 16
  lora_dropout: 0.1

# Training configuration - shorter for testing
training:
  epochs: 3  # Fewer epochs for faster testing
  lr: 0.01  # Higher LR for ResNet18
  momentum: 0.9
  weight_decay: 0.0001
  scheduler: none  # No scheduler for simplicity
  optimizer: sgd
  gradient_clip: 1.0

# Federated learning configuration
federated:
  num_rounds: 30  # Shorter runs for testing
  num_clients: 10
  client_fraction: 1.0  # Full participation
  aggregation_method: fedsa_shareA_dp  # Test DP-LoRA
  checkpoint_freq: 10

# Privacy configuration (DP-LoRA)
privacy:
  enable_privacy: true  # Test with DP enabled
  target: lora_A  # DP on A matrices only
  epsilon: 8.0  # Standard privacy budget
  delta: 1e-5
  max_grad_norm: 0.5
  noise_multiplier: 1.0

# Evaluation configuration
evaluation:
  eval_freq: 3  # Frequent evaluation for testing
  save_best_model: false  # Don't save models for testing
  metric: accuracy
  patience: null  # No early stopping

# Experiment configuration
experiment:
  name: DP_LoRA_Test
  output_dir: experiments/test_dp_lora
  save_model: false  # Don't save models for testing
  save_history: true
  log_interval: 5
  
  use_wandb: false
  wandb_project: dp-lora-test
  wandb_entity: null

# Reproducibility settings
reproducibility:
  deterministic: true  # Deterministic for reproducible testing

# Communication efficiency settings
communication:
  compress: false
  compression_ratio: 0.1
  quantization_bits: null

# Advanced settings
advanced:
  use_fbftl: false
  feature_extraction_rounds: 5
  personalized: true
  personalization_layers: ['classifier', 'lora_B']
  server_lr: 0.01
  server_momentum: 0.9