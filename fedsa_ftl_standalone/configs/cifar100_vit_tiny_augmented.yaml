# Configuration with stronger regularization and smaller ViT
# Adjusted to reduce overfitting on CIFAR-100

seed: 42
use_gpu: true

experiment:
  name: "fedsa_ftl_cifar100_vit_tiny"
  description: "FedSA-FTL with ViT-Tiny on CIFAR-100 - Strong regularization"
  output_dir: "experiments/vit_tiny_augmented"

model:
  num_classes: 100
  model_name: "vit_tiny"
  lora_r: 4          # Smaller rank for lower capacity
  lora_alpha: 8
  lora_dropout: 0.3  # Stronger dropout
  freeze_backbone: true
  pretrained: true

data:
  dataset_name: "cifar100"
  data_dir: "./data"
  batch_size: 32
  num_workers: 2
  data_split: "non_iid"
  alpha: 0.5
  model_type: "vit"
  
  augmentations:
    random_crop:
      enabled: true
      padding: 4
    horizontal_flip:
      enabled: true
      prob: 0.5
    color_jitter:
      enabled: true
      brightness: 0.2
      contrast: 0.2
      saturation: 0.2
      hue: 0.1
    random_rotation:
      enabled: true
      degrees: 15
    random_erasing:
      enabled: true
      prob: 0.25
      scale: [0.02, 0.33]
      ratio: [0.3, 3.3]
    mixup:
      enabled: true
      alpha: 0.2
      prob: 0.5
    cutmix:
      enabled: false
      alpha: 1.0
      prob: 0.5
  
  verbose: false

federated:
  num_clients: 10
  num_rounds: 100
  client_fraction: 0.3
  checkpoint_freq: 10
  save_best_model: true
  aggregation_method: "fedavg"

training:
  local_epochs: 5        # Increased for smaller ViT
  optimizer: "adamw"
  learning_rate: 0.0002  # Lower for stability
  weight_decay: 0.01     # Stronger regularization
  betas: [0.9, 0.999]
  eps: 1e-8
  grad_clip: 1.0
  accumulation_steps: 1
  label_smoothing: 0.1   # Added for generalization

privacy:
  enable_privacy: false

evaluation:
  eval_freq: 5
  eval_on_train: false
  eval_on_test: true
  save_predictions: false

reproducibility:
  deterministic: false
  benchmark: true