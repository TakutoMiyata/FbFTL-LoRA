advanced:
  feature_extraction_rounds: 10
  personalization_layers:
  - classifier
  personalized: false
  server_lr: 1.0
  server_momentum: 0.9
  use_fbftl: false

communication:
  compress: false
  compression_ratio: 0.1
  quantization_bits: null

data:
  alpha: 0.5
  dataset_name: cifar100
  data_split: non_iid
  input_size: 224
  imagenet_style: true
  batch_size: 256  # Further optimized for TITAN V (12GB VRAM)
  data_dir: ./data
  model_type: imagenet
  num_clients: 10
  num_workers: 4  # Reduced for better GPU utilization
  verbose: false
  augmentations:
    color_jitter:
      brightness: 0.2
      contrast: 0.2
      enabled: false
      hue: 0.1
      saturation: 0.2
    cutmix:
      alpha: 1.0
      enabled: false
      prob: 0.5
    horizontal_flip:
      enabled: true
      prob: 0.5
    mixup:
      alpha: 0.2
      enabled: false
      prob: 0.5
    random_crop:
      enabled: false
      padding: 4
    random_erasing:
      enabled: false
      prob: 0.5
    random_rotation:
      degrees: 15
      enabled: false
    random_resized_crop:
      enabled: true
      scale_min: 0.5

evaluation:
  eval_freq: 2
  metric: accuracy
  patience: 30
  save_best_model: true

experiment:
  log_interval: 10
  name: FedAvg_MobileNetV2_CIFAR100_IID
  output_dir: experiments/quickstart_resnet_fedavg_iid
  save_history: true
  save_model: true
  use_wandb: false
  wandb_entity: null
  wandb_project: fedavg-resnet

federated:
  aggregation_method: fedavg
  checkpoint_freq: 25
  client_fraction: 0.3
  num_clients: 10
  num_rounds: 100
  exclude_bn_from_agg: true

model:
  model_name: mobilenet_v2
  num_classes: 100
  pretrained: true
  freeze_backbone: false  # FedAvg: full fine-tuning
  lora:
    enabled: false  # No LoRA for baseline FedAvg
    r: 0
    alpha: 0
    dropout: 0.0

privacy:
  delta: 1.0e-05
  enable_privacy: false
  epsilon: 8.0
  max_grad_norm: 0.5
  noise_multiplier: 1.0
  target: all_params
  use_opacus_accounting: true

reproducibility:
  deterministic: false

seed: 42

training:
  epochs: 2
  lr: 0.005  # Scaled with larger batch size (256)
  weight_decay: 0.002
  scheduler: cosine
  warmup_epochs: 0
  label_smoothing: 0.1
  gradient_clip: 1.0
  optimizer: sgd
  momentum: 0.9
  microbatch_size: 8

use_gpu: true
use_amp: true