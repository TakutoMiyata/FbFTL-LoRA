# BiT (Big Transfer) with TFF CIFAR-100 Non-IID Configuration
# Follows experimental setup: 200 rounds, 25 round evaluation, 5-10 training clients
# Note: TFF uses same client IDs for train and test (each client has ~100 train + ~100 test samples)

seed: 42
use_gpu: true
use_amp: false  # Disable AMP for stability with BiT models

data:
  dataset_name: tff_cifar100
  data_dir: ./data
  num_clients: 10  # Training clients (5 or 10 as per paper)
  num_test_clients: 10  # Same as num_clients (TFF convention: same IDs for train/test)
  batch_size: 64
  num_workers: 4
  pin_memory: true
  input_size: 224  # BiT models use 224x224 input
  verbose: false
  augmentations:
    horizontal_flip:
      enabled: true
      prob: 0.5
    random_rotation:
      enabled: true
      degrees: 15
    random_resized_crop:
      enabled: true
      scale_min: 0.5
    color_jitter:
      enabled: false
      brightness: 0.2
      contrast: 0.2
      saturation: 0.2
      hue: 0.1
    random_erasing:
      enabled: false
      prob: 0.5
    mixup:
      enabled: false
      alpha: 0.2
      prob: 0.5
    cutmix:
      enabled: false
      alpha: 1.0
      prob: 0.5

model:
  model_name: bit_m_r50x1  # Options: bit_s_r50x1, bit_m_r50x1, bit_s_r101x1, bit_m_r101x1
  num_classes: 100
  pretrained: true  # Use BiT pretrained weights
  freeze_backbone: true  # Freeze backbone for transfer learning
  lora:
    enabled: true
    r: 8  # LoRA rank
    alpha: 16  # LoRA scaling factor
    dropout: 0.1

training:
  epochs: 5  # Local epochs per round
  lr: 0.001  # Learning rate
  momentum: 0.9
  weight_decay: 0.0001
  scheduler: cosine
  warmup_epochs: 0
  label_smoothing: 0.0
  gradient_clip: 1.0
  optimizer: sgd

federated:
  num_rounds: 200  # Total federated rounds (as per paper)
  num_clients: 10  # Training clients participating
  client_fraction: 1.0  # Fraction of clients selected per round (1.0 = all)
  aggregation_method: fedsa  # Options: fedsa, fedsa_shareA_dp
  checkpoint_freq: 25  # Save checkpoint every 25 rounds
  exclude_bn_from_agg: true

privacy:
  enable_privacy: false  # Set to true for DP-FedSA
  epsilon: 8.0
  delta: 1.0e-05
  max_grad_norm: 0.5
  noise_multiplier: 1.0
  target: lora_A  # Apply DP only to LoRA A matrices
  use_opacus_accounting: true

evaluation:
  eval_freq: 2  # Evaluate every 25 rounds (as per paper)
  metric: accuracy
  save_best_model: true

experiment:
  name: BiT_TFF_CIFAR100_NonIID_FedSA
  output_dir: experiments/quickstart_bit_tff
  save_history: true
  save_model: true
  log_interval: 10
  use_wandb: false
  wandb_project: bit-tff-fedsa
  wandb_entity: null

reproducibility:
  deterministic: false

advanced:
  personalized: true
  personalization_layers:
    - lora_B
    - head
  use_fbftl: false
  feature_extraction_rounds: 10
  server_lr: 1.0
  server_momentum: 0.9

communication:
  compress: false
  compression_ratio: 0.1
  quantization_bits: null
