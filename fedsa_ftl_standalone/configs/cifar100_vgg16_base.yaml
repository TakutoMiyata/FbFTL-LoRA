# FedSA-FTL Configuration for CIFAR-100 with VGG16
# Following FbFTL paper approach with frozen ImageNet pre-trained VGG16

# Random seed for reproducibility
seed: 42

# Use GPU if available
use_gpu: true

# Experiment settings
experiment:
  name: fedsa_ftl_cifar100_vgg16
  description: "FedSA-FTL with frozen VGG16 backbone and LoRA head on CIFAR-100"
  output_dir: experiments/fedsa_ftl_cifar100_vgg16

# Model configuration
model:
  num_classes: 100  # CIFAR-100 has 100 classes
  model_name: "vgg16"  # Pre-trained VGG16 model (FbFTL approach)
  lora_r: 16  # Moderate rank (proven configuration)
  lora_alpha: 16  # 1:1 ratio with r
  lora_dropout: 0.1  # Standard dropout for regularization
  freeze_backbone: true  # Freeze backbone (FbFTL approach)

# Data configuration
data:
  dataset_name: "cifar100"  # Use CIFAR-100 dataset
  data_dir: "./data"
  batch_size: 32
  num_workers: 2
  data_split: "non_iid"  # "iid" or "non_iid"
  alpha: 0.5  # Dirichlet distribution parameter for non-IID split
  verbose: true  # Print data distribution analysis

# Federated learning configuration
federated:
  num_clients: 10
  num_rounds: 100  # More rounds for CIFAR-100 complexity
  client_fraction: 0.3  # Fraction of clients selected per round
  checkpoint_freq: 10  # Save checkpoint every N rounds
  save_best_model: true
  aggregation_method: "fedavg"  # "fedavg" or "equal"

# Training configuration
training:
  local_epochs: 5
  learning_rate: 0.01  # Lower learning rate for 100 classes stability
  weight_decay: 0.0005  # Moderate regularization