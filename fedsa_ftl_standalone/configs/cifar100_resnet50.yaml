# Configuration for ResNet50 on CIFAR-100 with FedSA-FTL
# Non-IID federated learning with LoRA adapters

# Random seed for reproducibility
seed: 42

# Device configuration
use_gpu: true

# Data configuration
data:
  dataset_name: cifar100
  data_dir: ./data
  num_clients: 10
  data_split: non_iid  # 'iid' or 'non_iid'
  alpha: 0.5  # Dirichlet parameter for non-IID split (lower = more heterogeneous)
  batch_size: 64
  num_workers: 4
  model_type: resnet  # Important for correct data transforms
  verbose: false  # Set to true to see detailed data distribution
  
  # Data augmentation settings
  augmentations:
    # Standard augmentations
    horizontal_flip:
      enabled: true
      prob: 0.5
    
    random_rotation:
      enabled: true
      degrees: 10
    
    color_jitter:
      enabled: false  # Can enable for stronger augmentation
      brightness: 0.2
      contrast: 0.2
      saturation: 0.2
      hue: 0.1
    
    random_crop:
      enabled: false
      padding: 4
    
    random_erasing:
      enabled: false
      prob: 0.5
      scale: [0.02, 0.33]
      ratio: [0.3, 3.3]
    
    # Advanced augmentations
    mixup:
      enabled: false
      alpha: 0.2
      prob: 0.5
    
    cutmix:
      enabled: false
      alpha: 1.0
      prob: 0.5

# Model configuration
model:
  model_name: resnet50  # Options: resnet18, resnet34, resnet50, resnet101, resnet152
  num_classes: 100  # CIFAR-100 has 100 classes
  pretrained: true  # Use ImageNet pretrained weights
  
  # LoRA configuration
  lora_r: 8  # LoRA rank (smaller = less parameters)
  lora_alpha: 16  # LoRA scaling parameter
  lora_dropout: 0.1  # Dropout for LoRA layers

# Training configuration
training:
  epochs: 5  # Local epochs per round
  lr: 0.001  # Learning rate
  momentum: 0.9  # SGD momentum
  weight_decay: 0.0001  # L2 regularization
  scheduler: cosine  # Learning rate scheduler: 'none', 'step', 'cosine'
  warmup_epochs: 0  # Warmup epochs for scheduler
  
  # Optimizer settings
  optimizer: sgd  # Options: 'sgd', 'adam', 'adamw'
  
  # Gradient clipping
  gradient_clip: 1.0  # Max gradient norm (set to null to disable)
  microbatch_size: 8  # For DP per-sample clipping

# Federated learning configuration
federated:
  num_rounds: 100  # Number of federated rounds
  num_clients: 10  # Total number of clients
  client_fraction: 1.0  # Fraction of clients to select each round (1.0 = all clients)
  aggregation_method: fedsa_shareA_dp  # FedSA with DP on A matrices only
  checkpoint_freq: 20  # Save checkpoint every N rounds
  
  # FedProx settings (if using fedprox aggregation)
  fedprox_mu: 0.01  # Proximal term coefficient

# Privacy configuration (DP-LoRA for A matrices)
privacy:
  enable_privacy: true  # Enable differential privacy for A matrices
  target: lora_A  # Apply DP only to A matrices (shared)
  epsilon: 8.0  # Privacy budget
  delta: 1e-5  # Privacy parameter delta  
  max_grad_norm: 0.5  # Maximum gradient norm for clipping
  noise_multiplier: 1.0  # Noise multiplier for DP-SGD
  use_opacus_accounting: true  # Use Opacus RDPAccountant for accurate epsilon calculation

# Evaluation configuration
evaluation:
  eval_freq: 5  # Evaluate every N rounds
  save_best_model: true  # Save the best model based on validation accuracy
  metric: accuracy  # Metric to track: 'accuracy', 'loss'
  patience: 20  # Early stopping patience (set to null to disable)

# Experiment configuration
experiment:
  name: ResNet50_CIFAR100_NonIID
  output_dir: experiments/quickstart_resnet
  save_model: true  # Save final model
  save_history: true  # Save training history
  log_interval: 10  # Log every N batches
  
  # Wandb configuration (optional)
  use_wandb: false
  wandb_project: fedsa-ftl-resnet
  wandb_entity: null  # Your wandb username or team

# Reproducibility settings
reproducibility:
  deterministic: false  # Set to true for deterministic behavior (may reduce performance)
  
# Communication efficiency settings
communication:
  compress: false  # Enable gradient compression
  compression_ratio: 0.1  # Compression ratio (if compress is true)
  quantization_bits: null  # Number of bits for quantization (null = no quantization)

# Advanced settings
advanced:
  # Feature-based Federated Transfer Learning (FbFTL)
  use_fbftl: false  # Enable FbFTL mode
  feature_extraction_rounds: 10  # Rounds for feature extraction phase
  
  # Personalization
  personalized: true  # Enable personalized federated learning
  personalization_layers: ['classifier', 'lora_B']  # Layers to personalize
  
  # Server-side learning
  server_lr: 0.01  # Server learning rate for FedOpt variants
  server_momentum: 0.9  # Server momentum for FedOpt variants