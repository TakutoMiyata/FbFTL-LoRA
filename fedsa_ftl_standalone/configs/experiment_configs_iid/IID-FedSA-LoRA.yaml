advanced:
  feature_extraction_rounds: 10
  personalization_layers:
  - classifier
  - lora_B
  personalized: true
  server_lr: 1.0
  server_momentum: 0.9
  use_fbftl: false
communication:
  compress: false
  compression_ratio: 0.1
  quantization_bits: null
data:
  alpha: 0.5
  dataset_name: cifar100
  data_split: iid
  input_size: 224
  imagenet_style: true
  batch_size: 128
  data_dir: ./data
  model_type: imagenet
  num_clients: 10
  num_workers: 8
  pin_memory: true
  persistent_workers: true
  verbose: false
  augmentations:
    color_jitter:
      brightness: 0.2
      contrast: 0.2
      enabled: false
      hue: 0.1
      saturation: 0.2
    cutmix:
      alpha: 1.0
      enabled: false
      prob: 0.5
    horizontal_flip:
      enabled: true
      prob: 0.5
    mixup:
      alpha: 0.2
      enabled: false
      prob: 0.5
    random_crop:
      enabled: false
      padding: 4
    random_erasing:
      enabled: false
      prob: 0.5
    random_rotation:
      degrees: 15
      enabled: false
    random_resized_crop:
      enabled: true
      scale_min: 0.5
evaluation:
  eval_freq: 2
  metric: accuracy
  patience: 30
  save_best_model: true
experiment:
  log_interval: 10
  name: FedSA_LoRA_MobileNetV2_CIFAR100_IID
  output_dir: experiments/quickstart_resnet_iid
  save_history: true
  save_model: true
  use_wandb: false
  wandb_entity: null
  wandb_project: fedsa-ftl-resnet
federated:
  aggregation_method: fedsa
  checkpoint_freq: 25
  client_fraction: 0.3
  num_clients: 10
  num_rounds: 100
  exclude_bn_from_agg: true
model:
  model_name: mobilenet_v2
  num_classes: 100
  pretrained: true
  freeze_backbone: true
  lora:
    enabled: true
    r: 8
    alpha: 16
    dropout: 0.1
privacy:
  delta: 1.0e-05
  enable_privacy: false
  epsilon: 8.0
  max_grad_norm: 0.5
  noise_multiplier: 1.0
  target: lora_A
  use_opacus_accounting: true
reproducibility:
  deterministic: false
seed: 42
training:
  epochs: 2
  lr: 0.001  # Increased for faster convergence with LoRA and new classifier
  weight_decay: 0.001  # Reduced to allow better learning
  scheduler: cosine
  warmup_epochs: 0
  label_smoothing: 0.1
  gradient_clip: 1.0
  optimizer: sgd
  momentum: 0.9
  microbatch_size: 8
use_gpu: true
use_amp: false
