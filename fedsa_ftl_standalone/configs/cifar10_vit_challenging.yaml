# FedSA-FTL Configuration for CIFAR-10 with ViT - Challenging Non-IID Setting

# Random seed for reproducibility
seed: 42

# Use GPU if available
use_gpu: true

# Experiment settings
experiment:
  name: fedsa_ftl_cifar10_vit_challenging
  description: "FedSA-FTL with highly non-IID data distribution"
  output_dir: experiments/fedsa_ftl_cifar10_vit_challenging

# Model configuration
model:
  num_classes: 10
  model_name: "google/vit-base-patch16-224-in21k"  # Pre-trained ViT model
  lora_r: 4  # Lower rank for more compression
  lora_alpha: 8  # LoRA scaling factor
  lora_dropout: 0.1  # LoRA dropout
  freeze_backbone: true  # Freeze backbone (FbFTL approach)

# Data configuration
data:
  data_dir: "./data"
  batch_size: 32
  num_workers: 2
  data_split: "non_iid"  # "iid" or "non_iid"
  alpha: 0.1  # Lower alpha for more heterogeneous data
  verbose: true  # Print data distribution analysis

# Federated learning configuration
federated:
  num_clients: 20  # More clients
  num_rounds: 150
  client_fraction: 0.2  # Lower participation rate
  checkpoint_freq: 10
  save_best_model: true
  aggregation_method: "fedavg"

# Training configuration
training:
  local_epochs: 3  # Fewer local epochs
  learning_rate: 5e-4  # Lower learning rate
  weight_decay: 1e-4