# FedSA-FTL Configuration with Differential Privacy for CIFAR-10

# Random seed for reproducibility
seed: 42

# Use GPU if available
use_gpu: true

# Experiment settings
experiment:
  name: fedsa_ftl_cifar10_vit_private
  description: "FedSA-FTL with differential privacy on CIFAR-10"
  output_dir: experiments/fedsa_ftl_cifar10_vit_private

# Model configuration
model:
  num_classes: 10
  model_name: "google/vit-base-patch16-224-in21k"
  lora_r: 8
  lora_alpha: 16
  lora_dropout: 0.1
  freeze_backbone: true

# Data configuration
data:
  data_dir: "./data"
  batch_size: 32
  num_workers: 2
  data_split: "non_iid"
  alpha: 0.5
  verbose: true

# Privacy configuration
privacy:
  enable_privacy: true
  epsilon: 2.0  # Privacy budget (smaller = more private, typical range: 0.1-10)
  delta: 0.00001  # Privacy parameter for (ε,δ)-DP (avoid scientific notation in YAML)
  max_grad_norm: 1.0  # Maximum L2 norm for gradient clipping
  noise_multiplier: null  # Auto-computed from epsilon if null
  secure_aggregation: false  # Enable secure aggregation (additional overhead)

# Federated learning configuration
federated:
  num_clients: 10
  num_rounds: 50  # Fewer rounds due to privacy noise
  client_fraction: 0.3
  checkpoint_freq: 10
  save_best_model: true
  aggregation_method: "fedavg"

# Training configuration
training:
  local_epochs: 5
  learning_rate: 1e-3
  weight_decay: 1e-4