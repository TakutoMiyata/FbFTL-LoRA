# FedSA-FTL Configuration for CIFAR-100 with Vision Transformer (ViT)
# Following FbFTL paper approach with frozen ViT backbone and LoRA adaptation

# Random seed for reproducibility
seed: 42

# Use GPU if available
use_gpu: true

# Experiment settings
experiment:
  name: fedsa_ftl_cifar100_vit
  description: "FedSA-FTL with frozen ViT backbone and LoRA adaptation on CIFAR-100"
  output_dir: experiments/fedsa_ftl_cifar100_vit

# Model configuration
model:
  num_classes: 100  # CIFAR-100 has 100 classes
  model_name: "vit_small"  # ViT Small model for CIFAR-100
  lora_r: 16  # Moderate rank (consistent with VGG16)
  lora_alpha: 16  # 1:1 ratio with r
  lora_dropout: 0.1  # Standard dropout for regularization
  freeze_backbone: true  # Freeze backbone (FbFTL approach)

# Data configuration
data:
  dataset_name: "cifar100"  # Use CIFAR-100 dataset
  data_dir: "./data"
  batch_size: 32  # Smaller batch size for ViT memory efficiency
  num_workers: 2
  data_split: "non_iid"  # "iid" or "non_iid"
  alpha: 0.5  # Dirichlet distribution parameter for non-IID split
  verbose: true  # Print data distribution analysis
  model_type: "vit"  # Use ViT-specific transforms (32x32 images)

# Federated learning configuration
federated:
  num_clients: 10
  num_rounds: 100  # More rounds for ViT convergence
  client_fraction: 0.3  # Fraction of clients selected per round
  checkpoint_freq: 10  # Save checkpoint every N rounds
  save_best_model: true
  aggregation_method: "fedavg"  # "fedavg" or "equal"

# Training configuration
training:
  local_epochs: 5
  optimizer: 'adamw'  # Use AdamW optimizer for ViT
  learning_rate: 0.0001  # 推奨学習率 (1e-4)
  weight_decay: 0.01  # AdamW向けに調整
  betas: [0.9, 0.999]  # Adam beta parameters
  eps: 1e-8  # Adam epsilon parameter

# Privacy configuration (まずはDPなしでベースライン性能を確認)
privacy:
  enable_privacy: false  # ★★★ まずはDPなしでベースライン性能を確認 ★★★
  epsilon: 8.0  # Privacy budget
  delta: 1e-5  # Privacy parameter
  max_grad_norm: 1.0  # Gradient clipping for differential privacy
  noise_multiplier: null  # Auto-calculated from epsilon/delta
  secure_aggregation: false  # Enable secure aggregation
  total_rounds: 100  # ラウンド数と合わせる

# Logging and monitoring
logging:
  log_level: "INFO"  # "DEBUG", "INFO", "WARNING", "ERROR"
  save_logs: true
  tensorboard: false  # Enable TensorBoard logging
  wandb: false  # Enable Weights & Biases logging
  
# Evaluation settings
evaluation:
  eval_freq: 5  # Evaluate every N rounds
  eval_on_train: false  # Evaluate on training data
  eval_on_test: true  # Evaluate on test data
  save_predictions: false  # Save predictions for analysis

# Early stopping (optional)
# early_stopping:
#   patience: 20  # Stop if no improvement for N rounds
#   min_delta: 0.01  # Minimum improvement threshold
#   metric: "test_accuracy"  # Metric to monitor

# Resource constraints
resources:
  max_memory_gb: null  # Maximum memory usage (null for no limit)
  max_time_hours: null  # Maximum training time (null for no limit)

# Reproducibility settings
reproducibility:
  deterministic: false  # Use deterministic algorithms (may be slower)
  benchmark: true  # Use cudnn benchmark for consistent input sizes